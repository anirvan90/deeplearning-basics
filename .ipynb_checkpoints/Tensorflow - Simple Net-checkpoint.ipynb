{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow - A Simple Neural Net with TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.framework import ops\n",
    "from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Compute Loss For 1 Training Example\n",
    "\n",
    "The loss function we assume here is:\n",
    "$$loss = \\mathcal{L}(\\hat{y}, y) = (\\hat y^{(i)} - y^{(i)})^2 \\tag{1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "y_hat = tf.constant(36, name='y_hat') ## Declare a constant y-hat\n",
    "y = tf.constant(39, name='y') ## Declare another constant y\n",
    "\n",
    "loss = tf.Variable((y-y_hat)**2, name='loss') ## Implement the loss function as described above\n",
    "\n",
    "init = tf.global_variables_initializer() ## Create a loss variable\n",
    "\n",
    "with tf.Session() as sess: ## Create Session\n",
    "    sess.run(init)         ## Run session with init and output the initialized loss variable\n",
    "    print(sess.run(loss))  ## Print loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simpler Example - Basic Math\n",
    "\n",
    "TensorFlow creates tensors (variables) but does not compute output till sessions are initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mul:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(5)\n",
    "b = tf.constant(2)\n",
    "c = tf.multiply(a,b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expected, the print output of c is a tensor that contains the multiply operation. We need to create a session, run the session to see our expected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholders\n",
    "\n",
    "A placeholder is just another varibale whose value we can assign later: ie when the session is run. We assign the value with the 'feed_dict' parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.int64, name='x')\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "print(sess.run(2 * x, feed_dict={x:10}))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Linear Function\n",
    "Compute a linear function:\n",
    "\\begin{equation}\n",
    "Y = WX + b\n",
    "\\end{equation}\n",
    "\n",
    "Where W and X are random matrices and b is a random vector\n",
    "\n",
    "Lets assume the following shapes for W, X and b:\n",
    "W - (4,3)\n",
    "X - (3,1)\n",
    "b - (4,1)\n",
    "\n",
    "Use numpy for random matrices and tensorflow to create tensors for the created matrices/vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_function():\n",
    "    \"\"\"\n",
    "    Implements a linear function: \n",
    "            Initializes W to be a random tensor of shape (4,3)\n",
    "            Initializes X to be a random tensor of shape (3,1)\n",
    "            Initializes b to be a random tensor of shape (4,1)\n",
    "    Returns: \n",
    "    result -- runs the session for Y = WX + b \n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    ## Tensor Constants\n",
    "    X = tf.constant(np.random.randn(3,1), name = 'X')\n",
    "    W = tf.constant(np.random.randn(4,3), name = 'W')\n",
    "    b = tf.constant(np.random.randn(4,1), name = 'b')\n",
    "    \n",
    "    ## Tensor Function/Operation\n",
    "    Y = tf.add(tf.matmul(W,X), b)\n",
    "    \n",
    "    ## Create Session and Run\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    ## Store result from session\n",
    "    result = sess.run(Y)\n",
    "    \n",
    "    ## Close Session\n",
    "    sess.close()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result = [[-2.15657382]\n",
      " [ 2.95891446]\n",
      " [-1.08926781]\n",
      " [-0.84538042]]\n"
     ]
    }
   ],
   "source": [
    "print('Result = '+ str(linear_function()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Function\n",
    "\n",
    "TensorFlow has built in functions we use in our deeplearning applications like sigmoid, softmax, relu etc etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Computes sigmoid of z\n",
    "    \n",
    "    Arguments:\n",
    "    z -- input, vector or scalar\n",
    "    \n",
    "    Returns:\n",
    "    result -- the sigmoid of z\n",
    "    \"\"\"\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, name='x')\n",
    "    \n",
    "    sigmoid = tf.sigmoid(x)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        result = sess.run(sigmoid, feed_dict={x:z})\n",
    "        sess.close()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(0): 0.5\n",
      "sigmoid(2): 0.880797\n"
     ]
    }
   ],
   "source": [
    "print(\"sigmoid(0): \"+ str(sigmoid(0)))\n",
    "print(\"sigmoid(2): \"+ str(sigmoid(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute The Cross-Entropy Cost Function\n",
    "With TensorFlow you can compute the cross entropy cost function with just one line of code. Instead computing each step of the equation, let TF do the heavy lifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(logits, labels):\n",
    "    \"\"\"\n",
    "    Computes the cost using the sigmoid cross entropy\n",
    "    \n",
    "    Arguments:\n",
    "    logits -- vector containing z, output of the last linear unit (before the final sigmoid activation)\n",
    "    labels -- vector of labels y (1 or 0) \n",
    "    \n",
    "    Note: What we've been calling \"z\" and \"y\" in this class are respectively called \"logits\" and \"labels\" \n",
    "    in the TensorFlow documentation. So logits will feed into z, and labels into y. \n",
    "    \n",
    "    Returns:\n",
    "    cost -- runs the session of the cost (formula (2))\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Create Placeholders for logits(z) and labels(y)\n",
    "    z = tf.placeholder(tf.float32, name = 'z')\n",
    "    y = tf.placeholder(tf.float32, name = 'y')\n",
    "    \n",
    "    ## Compute Sigmoid Cross-Entropy Cost\n",
    "    cost = tf.nn.sigmoid_cross_entropy_with_logits(logits = z, labels = y)\n",
    "    \n",
    "    ## Create Session, Run & Close\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    cost = sess.run(cost, feed_dict={z:logits, y:labels})\n",
    "    \n",
    "    sess.close()\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = [ 1.00538719  1.03664088  0.41385433  0.39956614]\n"
     ]
    }
   ],
   "source": [
    "logits = sigmoid(np.array([0.2, 0.4, 0.7, 0.9]))\n",
    "labels = np.array([0,0,1,1])\n",
    "cost = cost(logits, labels)\n",
    "print(\"cost = \"+str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding\n",
    "Many a time we have a vector of classes, with each class represented as a number or text. To simplify our operations we can convert that vector into a one hot matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_matrix(labels, C):\n",
    "    \"\"\"\n",
    "    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n",
    "                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) \n",
    "                     will be 1. \n",
    "                     \n",
    "    Arguments:\n",
    "    labels -- vector containing the labels \n",
    "    C -- number of classes, the depth of the one hot dimension\n",
    "    \n",
    "    Returns: \n",
    "    one_hot -- one hot matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    C = tf.constant(C, name='C')\n",
    "    \n",
    "    one_hot_matrix = tf.one_hot(indices=labels, depth=C, axis=0)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        one_hot = sess.run(one_hot_matrix)\n",
    "        sess.close()\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  1.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([1,2,3,0,2,1])\n",
    "C = 4\n",
    "one_hot = one_hot_matrix(labels, C)\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Matrices and Vectors\n",
    "TensorFlow is built in support for initialization as well. tf.ones() for ones and tf.zeros() for zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ones(shape):\n",
    "    \"\"\"\n",
    "    Creates an array of ones of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    shape -- shape of the array you want to create\n",
    "        \n",
    "    Returns: \n",
    "    ones -- array containing only ones\n",
    "    \"\"\"\n",
    "    \n",
    "    ones = tf.ones(shape)\n",
    "    with tf.Session() as sess:\n",
    "        output = sess.run(ones)\n",
    "        sess.close()\n",
    "    \n",
    "    return output\n",
    "\n",
    "def zeros(shape):\n",
    "    \"\"\"\n",
    "    Creates an array of zeros of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    shape -- shape of the array you want to create\n",
    "        \n",
    "    Returns: \n",
    "    ones -- array containing only zeros\n",
    "    \"\"\"\n",
    "    \n",
    "    zeros = tf.zeros(shape)\n",
    "    with tf.Session() as sess:\n",
    "        output = sess.run(zeros)\n",
    "        sess.close()\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones: [ 1.  1.  1.]\n",
      "Zeros: [[ 0.  0.]\n",
      " [ 0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Ones: \"+str(ones([3])))\n",
    "print(\"Zeros: \"+str(zeros([2,2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Building A Simple Net With What We Know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 2\n",
      "\n",
      "\n",
      "X-Train (1080, 64, 64, 3)\n",
      "Y_Train (1, 1080)\n",
      "[[0 0 0 5 1 0 3 1 5 1 5 1 3 1 1 3 5 4 0 4 5 4 2 5 3 5 4 2 1 2 3 1 0 3 1 1 0\n",
      "  4 2 3 0 3 0 2 3 1 2 2 0 3 4 1 2 0 4 0 4 0 4 4 5 5 2 4 4 5 0 1 3 5 0 4 1 2\n",
      "  3 4 3 5 1 5 2 0 1 4 2 4 4 1 4 5 5 0 0 5 5 5 3 3 5 2 2 2 0 2 5 3 0 2 3 4 1\n",
      "  3 2 4 2 2 1 3 1 3]]\n",
      "X_Test (120, 64, 64, 3)\n",
      "Y_Test (1, 120)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDP591gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x182cd83198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Load the dataset and take a peek at some of the data\n",
    "\n",
    "X_train_orig, Y_train_orig,X_test_orig, Y_test_orig, classes = load_dataset()\n",
    "\n",
    "## Example of A picture in the dataset\n",
    "index = 4\n",
    "plt.imshow(X_train_orig[index])\n",
    "print (\"y = \"+str(np.squeeze(Y_train_orig[:,index])))\n",
    "print(\"\\n\")\n",
    "\n",
    "## Shapes of our data\n",
    "print(\"X-Train\", str(X_train_orig.shape))\n",
    "print(\"Y_Train\", str(Y_train_orig.shape))\n",
    "print(Y_test_orig)\n",
    "print(\"X_Test\", str(X_test_orig.shape))\n",
    "print(\"Y_Test\", str(Y_test_orig.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see we have 1080 train images and 120 test images, each 64x64 and 3 color channels\n",
    "Our label vectors for each correspond to the number of training and test examples\n",
    "\n",
    "\n",
    "As usual we need to flatten/unroll our images and normalize them by dividing by 255 to get more manageable numbers. In addtion we convert each label to a one-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-Train-Flat: (12288, 1080)\n",
      "X-Test-Flat: (12288, 120)\n",
      "\n",
      "\n",
      "Y-Train-OneHot :(6, 1080)\n",
      "Y-Test-OneHot: (6, 120)\n",
      "[[ 1.  1.  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "   1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.\n",
      "   0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  1.  0.  1.  0.  1.  0.  1.  1.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  1.  1.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  1.  1.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  0.  1.  0.  0.  0.  1.\n",
      "   0.  0.  0.  0.  1.  0.  1.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.\n",
      "   0.  0.  0.  1.  0.  1.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "   0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "   1.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  1.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "   1.  0.  1.  0.  1.  1.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  1.  0.  1.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.  1.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.\n",
      "   0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   1.  0.  0.  1.  1.  1.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "## Unroll Images\n",
    "X_train_flatten = X_train_orig.reshape(X_train_orig.shape[0], -1).T\n",
    "X_test_flatten = X_test_orig.reshape(X_test_orig.shape[0], -1).T\n",
    "\n",
    "## Normalize Images\n",
    "X_train = X_train_flatten / 255\n",
    "X_test = X_test_flatten / 255\n",
    "\n",
    "## Convert Labels to One-Hot\n",
    "Y_train = convert_to_one_hot(Y_train_orig, 6)\n",
    "Y_test = convert_to_one_hot(Y_test_orig, 6)\n",
    "\n",
    "print(\"X-Train-Flat: \"+str(X_train.shape))\n",
    "print(\"X-Test-Flat: \"+str(X_test.shape))\n",
    "print(\"\\n\")\n",
    "print(\"Y-Train-OneHot :\"+str(Y_train.shape))\n",
    "print(\"Y-Test-OneHot: \"+str(Y_test.shape))\n",
    "\n",
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new image shapes represent the unrolled images 12288 pixel inputs = (64*64*3) and 1080/120 examples\n",
    "The new label shapes represent the one-hot vectors 6 possible labels for each example 1080/120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions To Build The Model\n",
    "\n",
    "The model we are building will have the following layers \n",
    "Linear -> ReLU -> Linear -> ReLU -> Linear -> Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Placeholders\n",
    "\n",
    "We need to create placeholder for X and Y. We use these to pass in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n",
    "    n_y -- scalar, number of classes (from 0 to 5, so -> 6)\n",
    "    \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
    "    \"\"\"\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, [n_x, None], name='X')\n",
    "    Y = tf.placeholder(tf.float32, [n_y, None], name='Y')\n",
    "    \n",
    "    ## We use None in when declaring the shape because the number of training examples and test examples are\n",
    "    ## different, this makes our function a little more flexible\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = create_placeholders(12288, 6)\n",
    "\n",
    "print(\"X = \"+str(X))\n",
    "print(\"Y = \"+str(Y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good sanity check, we expect X & Y to be tensors of 12288, None and 6, None shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Parameters\n",
    "We need to create a dictionary of parameters. We will use Xavier Initialization for weights and zero initialization for bias vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [25, 12288]\n",
    "                        b1 : [25, 1]\n",
    "                        W2 : [12, 25]\n",
    "                        b2 : [12, 1]\n",
    "                        W3 : [6, 12]\n",
    "                        b3 : [6, 1]\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)\n",
    "    \n",
    "    W1 = tf.get_variable(\"W1\", [25, 12288], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b1 = tf.get_variable(\"b1\", [25, 1], initializer = tf.zeros_initializer())\n",
    "    W2 = tf.get_variable(\"W2\", [12, 25], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b2 = tf.get_variable(\"b2\", [12, 1], initializer = tf.zeros_initializer())\n",
    "    W3 = tf.get_variable(\"W3\", [6, 12], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b3 = tf.get_variable(\"b3\", [6, 1], initializer = tf.zeros_initializer())\n",
    "    \n",
    "    parameters = {\n",
    "        \"W1\":W1,\n",
    "        \"b1\":b1,\n",
    "        \"W2\":W2,\n",
    "        \"b2\":b2,\n",
    "        \"W3\":W3,\n",
    "        \"b3\":b3\n",
    "    }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    parameters = initialize_parameters()\n",
    "    print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "    print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "    print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "    print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation With TF\n",
    "Refresher:\n",
    "- Linear = WX + b\n",
    "- Relu = Relu(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    Z1 = tf.add(tf.matmul(W1, X), b1)\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2, A1), b2)\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(W3, A2), b3)\n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(12288, 6)\n",
    "    parameters = initialize_parameters()\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    print(\"Z3 = \"+str(Z3))\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Cost\n",
    "Computing cost is simple with TensorFlow, just use tf.nn.softmax_cross_entropy_with_logits() or something similar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(12288,6)\n",
    "    parameters = initialize_parameters()\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    print(\"Cost = \"+ str(cost))\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation and Parameter Update\n",
    "\n",
    "TensorFlow takes care of this with the optimizer object. After the cost function is computed we can call the optimizer function\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).reduce(cost)\n",
    "\n",
    "And to make the optimization \n",
    "\n",
    "_ , c = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together - Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "         num_epochs = 1500, minibatch_size=32, print_cost = True):\n",
    "        \n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n",
    "    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n",
    "    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n",
    "    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()\n",
    "    tf.set_random_seed(1)\n",
    "    seed = 3\n",
    "    (n_x, m) = X_train.shape\n",
    "    n_y = Y_train.shape[0]\n",
    "    costs = []\n",
    "    \n",
    "    ## Placeholders\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "    \n",
    "    ## Param Initialization\n",
    "    parameters = initialize_parameters()\n",
    "    \n",
    "    ## Forward Prop\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    \n",
    "    ## Cost Function\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    \n",
    "    ## Optimizer/Back Prop\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Initialize Variables and Session\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            epoch_cost = 0.\n",
    "            num_minibatches = int(m/minibatch_size)\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "            \n",
    "            for minibatch in minibatches:\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                _, minibatch_cost = sess.run([optimizer, cost], feed_dict={X:minibatch_X, Y:minibatch_Y})\n",
    "    \n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "            \n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print(\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "    \n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = model(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading Our Own Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n",
    "my_image = \"fighton.jpg\"\n",
    "\n",
    "fname = \"images/\"+my_image\n",
    "image = np.array(ndimage.imread(fname, flatten=False))\n",
    "my_image = scipy.misc.imresize(image, size=(64, 64)).reshape((1, 64 * 64 * 3)).T\n",
    "my_image_prediction = predict(my_image, parameters)\n",
    "\n",
    "plt.imshow(image)\n",
    "print(\"Your algorithm predicts: y = \" + str(np.squeeze(my_image_prediction)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- TensorFlow does a lot of the heavy lifting by abstracting away some steps and many lines of code\n",
    "- Our model fits the training set well but performs relatively poorly on the test set - high variance\n",
    "- We could optimize by using regularization (dropout or L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

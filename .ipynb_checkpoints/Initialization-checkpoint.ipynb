{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Initialization\n",
    "Random initialization and zero initialization for weight and bias parameters. A well chosen initialization can speed\n",
    "up gradient descent and increase the odds of gradient descent converging to a lower training error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three Kinds of Initialization Patterns\n",
    "- Zeros\n",
    "- Random\n",
    "- He"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeros Initialization\n",
    "There are two parameters that need to be initialized\n",
    "- Weight matrices\n",
    "- Bias vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_zeros(layers_dims):\n",
    "    \"\"\"\n",
    "    Initializes weights and biases to zeros for each layer in the network\n",
    "    \n",
    "    Arguments:\n",
    "    layers_dims -- a python list/array containing the size of each layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a python dictionary containing parameters \"W1\", \"b1\", ... \"WL\", \"bL\"\n",
    "                  \n",
    "                  \"W1\" -- weight matrix of shape (layers_dims[1], layers_dims[0])\n",
    "                  \"b1\" -- bias vector of shape (layers_dims[1], 0)\n",
    "                  \"WL\" -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n",
    "                  \"bL\" -- bias vector of shape (layers_dims[L], 0)\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(layers_dims)\n",
    "    print(\"L: \" + str(L))\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters[\"W\" + str(l)] = np.zeros((layers_dims[l], layers_dims[l-1]))\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L: 3\n",
      "(2, 3)\n",
      "(2, 1)\n",
      "(1, 2)\n",
      "(1, 1)\n",
      "(2, 3)\n",
      "(2, 1)\n",
      "(1, 2)\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_zeros([3,2,1])\n",
    "print(parameters[\"W1\"].shape)\n",
    "print(parameters[\"b1\"].shape)\n",
    "print(parameters[\"W2\"].shape)\n",
    "print(parameters[\"b2\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Initialization\n",
    "Here we initialize the weight matrice to random numbers to reduce symmetry so each neuron can learn a different function of its inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_random(layers_dims):\n",
    "    \"\"\"\n",
    "    Initializes weights to random numbers and biases to zeros for each layer in the network\n",
    "    \n",
    "    Arguments:\n",
    "    layers_dims -- a python list/array containing the size of each layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a python dictionary containing parameters \"W1\", \"b1\", ... \"WL\", \"bL\"\n",
    "                  \n",
    "                  \"W1\" -- weight matrix of shape (layers_dims[1], layers_dims[0])\n",
    "                  \"b1\" -- bias vector of shape (layers_dims[1], 0)\n",
    "                  \"WL\" -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n",
    "                  \"bL\" -- bias vector of shape (layers_dims[L], 0)\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    L = len(layers_dims)\n",
    "    \n",
    "    for l in range(1,L):\n",
    "        parameters[\"W\"+str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1])*100\n",
    "        parameters[\"b\"+str(l)] = np.zeros((layers_dims[l], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[  39.44652651,  113.81267306, -197.69783122,  134.52115664],\n",
      "       [ -48.5511674 , -146.43350257,   27.55507781,  -40.72846812],\n",
      "       [ -54.32649667, -236.51869032,  -59.78637922,  -85.53514902]]), 'b1': array([[ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.]]), 'W2': array([[-73.90643561,  68.636284  , -27.28415422],\n",
      "       [-40.38758833,  42.63536627, -96.9557333 ]]), 'b2': array([[ 0.],\n",
      "       [ 0.]]), 'W3': array([[-48.96254441, -81.02019568]]), 'b3': array([[ 0.]])}\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_random([3,2,1])\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### He Initialization\n",
    "Similar to Xavier Initialization except Xavier uses a scaling factor for weight where as He uses a Square Root\n",
    "$\\sqrt{\\frac{2}{\\text{dimension of the previous layer}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_he(layers_dims):\n",
    "    \"\"\"\n",
    "    Initializes weights to random numbers and biases to zeros for each layer in the network\n",
    "    \n",
    "    Arguments:\n",
    "    layers_dims -- a python list/array containing the size of each layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a python dictionary containing parameters \"W1\", \"b1\", ... \"WL\", \"bL\"\n",
    "                  \n",
    "                  \"W1\" -- weight matrix of shape (layers_dims[1], layers_dims[0])\n",
    "                  \"b1\" -- bias vector of shape (layers_dims[1], 0)\n",
    "                  \"WL\" -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n",
    "                  \"bL\" -- bias vector of shape (layers_dims[L], 0)\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    L = len(layers_dims)\n",
    "    \n",
    "    for l in range(1,L):\n",
    "        parameters[\"W\"+str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1])* np.sqrt(2/layers_dims[l-1])\n",
    "        parameters[\"b\"+str(l)] = np.zeros((layers_dims[l], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[ 0.96853535, -1.13612556,  0.10729257],\n",
      "       [ 0.57502473,  0.24825157,  0.17108535]]), 'b1': array([[ 0.],\n",
      "       [ 0.]]), 'W2': array([[ 0.59034135, -1.23267981]]), 'b2': array([[ 0.]])}\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_he([3,2,1])\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

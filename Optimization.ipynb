{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Algorithms\n",
    "\n",
    "We use an optimization method to update parameters and minimize the cost function J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import math\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "from opt_utils import load_params_and_grads, initialize_parameters, forward_propagation, backward_propagation\n",
    "from opt_utils import compute_cost, predict, predict_dec, plot_decision_boundary, load_dataset\n",
    "from testCases import *\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Gradient Descent\n",
    "\n",
    "Simple gradient descent updates parameters using the derivative of the layer multiplied by a learning_rate \n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{1}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters_with_gd(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Upadate params with one step of Gradient Descent\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    parameters -- Python dicitionary containing the weight matrices and bias vectors that need to be updated\n",
    "    \n",
    "    grads -- Python dictionary containing gradients for each weight matrix and bias vector\n",
    "    \n",
    "    learning_rate -- the learning_rate and which we want to go down, a scalar value\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- Python dictionary with updated parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    for l in range(L):\n",
    "        parameters[\"W\"+str(l+1)] = parameters[\"W\"+str(l+1)] - (learning_rate * grads[\"dW\"+str(l+1)])\n",
    "        parameters[\"b\"+str(l+1)] = parameters[\"b\"+str(l+1)] - (learning_rate) * grads[\"db\"+str(l+1)]\n",
    "    \n",
    "    return parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.63535156 -0.62320365 -0.53718766]\n",
      " [-1.07799357  0.85639907 -2.29470142]]\n",
      "[[ 1.74604067]\n",
      " [-0.75184921]]\n",
      "[[ 0.32171798 -0.25467393  1.46902454]\n",
      " [-2.05617317 -0.31554548 -0.3756023 ]\n",
      " [ 1.1404819  -1.09976462 -0.1612551 ]]\n",
      "[[-0.88020257]\n",
      " [ 0.02561572]\n",
      " [ 0.57539477]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads, learning_rate = update_parameters_with_gd_test_case()\n",
    "\n",
    "params= update_parameters_with_gd(parameters, grads, learning_rate)\n",
    "print(params[\"W1\"])\n",
    "print(params[\"b1\"])\n",
    "print(params[\"W2\"])\n",
    "print(params[\"b2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam Optimization\n",
    "\n",
    "Combines the ideas of RMS Prop and Momentum Gradient Descent\n",
    "\n",
    "1. It calculates an exponentially weighted average of past gradients, and stores it in variables $v$ (before bias correction) and $v^{corrected}$ (with bias correction). \n",
    "2. It calculates an exponentially weighted average of the squares of the past gradients, and  stores it in variables $s$ (before bias correction) and $s^{corrected}$ (with bias correction). \n",
    "3. It updates parameters in a direction based on combining information from \"1\" and \"2\".\n",
    "\n",
    "\n",
    "$$\\begin{cases}\n",
    "v_{dW^{[l]}} = \\beta_1 v_{dW^{[l]}} + (1 - \\beta_1) \\frac{\\partial \\mathcal{J} }{ \\partial W^{[l]} } \\\\\n",
    "v^{corrected}_{dW^{[l]}} = \\frac{v_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\\n",
    "s_{dW^{[l]}} = \\beta_2 s_{dW^{[l]}} + (1 - \\beta_2) (\\frac{\\partial \\mathcal{J} }{\\partial W^{[l]} })^2 \\\\\n",
    "s^{corrected}_{dW^{[l]}} = \\frac{s_{dW^{[l]}}}{1 - (\\beta_2)^t} \\\\\n",
    "W^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{dW^{[l]}}}{\\sqrt{s^{corrected}_{dW^{[l]}}} + \\varepsilon}\n",
    "\\end{cases}$$\n",
    "\n",
    "where:\n",
    "- t counts the number of steps taken of Adam \n",
    "- L is the number of layers\n",
    "- $\\beta_1$ and $\\beta_2$ are hyperparameters that control the two exponentially weighted averages. \n",
    "- $\\alpha$ is the learning rate\n",
    "- $\\varepsilon$ is a very small number to avoid dividing by zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_adam(parameters):\n",
    "    \"\"\"\n",
    "    Initializes v and s as two python dictionaries with:\n",
    "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
    "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters.\n",
    "                    parameters[\"W\" + str(l)] = Wl\n",
    "                    parameters[\"b\" + str(l)] = bl\n",
    "    \n",
    "    Returns: \n",
    "    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n",
    "                    v[\"dW\" + str(l)] = ...\n",
    "                    v[\"db\" + str(l)] = ...\n",
    "    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n",
    "                    s[\"dW\" + str(l)] = ...\n",
    "                    s[\"db\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\"+str(l+1)] = np.zeros_like(parameters[\"W\"+str(l+1)])\n",
    "        v[\"db\"+str(l+1)] = np.zeros_like(parameters[\"b\"+str(l+1)])\n",
    "        s[\"dW\"+str(l+1)] = np.zeros_like(parameters[\"W\"+str(l+1)])\n",
    "        s[\"db\"+str(l+1)] = np.zeros_like(parameters[\"b\"+str(l+1)])\n",
    "    \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dW1': array([[ 0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.]]), 'db1': array([[ 0.],\n",
      "       [ 0.]]), 'dW2': array([[ 0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.]]), 'db2': array([[ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.]])}\n",
      "{'dW1': array([[ 0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.]]), 'db1': array([[ 0.],\n",
      "       [ 0.]]), 'dW2': array([[ 0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.]]), 'db2': array([[ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.]])}\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_adam_test_case()\n",
    "v, s = initialize_adam(parameters)\n",
    "print(v)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Update Paramteters With Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
    "                                beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Update parameters using Adam\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients for each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "    t -- Number of steps taken of Adam\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    beta1 -- Exponential decay hyperparameter for the first moment estimates \n",
    "    beta2 -- Exponential decay hyperparameter for the second moment estimates \n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    v_corrected = {}\n",
    "    s_corrected = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        ## Moving average of gradients\n",
    "        v[\"dW\"+str(l+1)] = beta1 * v[\"dW\"+str(l+1)] + (1-beta1)*grads[\"dW\"+str(l+1)]\n",
    "        v[\"db\"+str(l+1)] = beta1 * v[\"db\"+str(l+1)] + (1-beta1)*grads[\"db\"+str(l+1)]\n",
    "        \n",
    "        ## Compute bias corrected first moment estimate\n",
    "        v_corrected[\"dW\"+str(l+1)] = v[\"dW\"+str(l+1)] / (1-np.power(beta1, t))\n",
    "        v_corrected[\"db\"+str(l+1)] = v[\"db\"+str(l+1)] / (1-np.power(beta1, t))\n",
    "        \n",
    "        ## Moving average of squared gradients\n",
    "        s[\"dW\"+str(l+1)] = beta2 * s[\"dW\"+str(l+1)] / (1-beta2)*np.power(grads[\"dW\"+str(l+1)], 2)\n",
    "        s[\"db\"+str(l+1)] = beta2 * s[\"db\"+str(l+1)] / (1-beta2)*np.power(grads[\"db\"+str(l+1)], 2)\n",
    "        \n",
    "        ## Bias corrected of second raw moment\n",
    "        s_corrected[\"dW\"+str(l+1)] = s[\"dW\"+str(l+1)] / (1 - np.power(beta2, t))\n",
    "        s_corrected[\"db\"+str(l+1)] = s[\"db\"+str(l+1)] / (1 - np.power(beta2, t))\n",
    "        \n",
    "        ## Update Parameters\n",
    "        parameters[\"W\"+str(l+1)] = parameters[\"W\"+str(l+1)] - learning_rate * v_corrected[\"dW\"+str(l+1)]/np.sqrt(s_corrected[\"dW\"+str(l+1)]+epsilon)\n",
    "        parameters[\"b\"+str(l+1)] = parameters[\"b\"+str(l+1)] - learning_rate * v_corrected[\"db\"+str(l+1)]/np.sqrt(s_corrected[\"db\"+str(l+1)]+epsilon)\n",
    "        \n",
    "    \n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 59.55167048 -60.86037272 -47.98031494]\n",
      " [-27.52003909 -46.54806338  33.6841381 ]]\n",
      "b1 = [[  8.21271837]\n",
      " [ 48.48981595]]\n",
      "W2 = [[ 14.41841171 -28.16281599  37.8653054 ]\n",
      " [ 18.82162386  35.84456701  44.10045309]\n",
      " [ 36.46251317  -0.43333343  58.63337962]]\n",
      "b2 = [[-13.21552672]\n",
      " [-87.31579557]\n",
      " [-38.47214061]]\n",
      "v[\"dW1\"] = [[-0.11006192  0.11447237  0.09015907]\n",
      " [ 0.05024943  0.09008559 -0.06837279]]\n",
      "v[\"db1\"] = [[-0.01228902]\n",
      " [-0.09357694]]\n",
      "v[\"dW2\"] = [[-0.02678881  0.05303555 -0.06916608]\n",
      " [-0.03967535 -0.06871727 -0.08452056]\n",
      " [-0.06712461 -0.00126646 -0.11173103]]\n",
      "v[\"db2\"] = [[ 0.02344157]\n",
      " [ 0.16598022]\n",
      " [ 0.07420442]]\n",
      "s[\"dW1\"] = [[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "s[\"db1\"] = [[ 0.]\n",
      " [ 0.]]\n",
      "s[\"dW2\"] = [[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "s[\"db2\"] = [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads, v, s = update_parameters_with_adam_test_case()\n",
    "parameters, v, s  = update_parameters_with_adam(parameters, grads, v, s, t = 2)\n",
    "\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "print(\"v[\\\"dW1\\\"] = \" + str(v[\"dW1\"]))\n",
    "print(\"v[\\\"db1\\\"] = \" + str(v[\"db1\"]))\n",
    "print(\"v[\\\"dW2\\\"] = \" + str(v[\"dW2\"]))\n",
    "print(\"v[\\\"db2\\\"] = \" + str(v[\"db2\"]))\n",
    "print(\"s[\\\"dW1\\\"] = \" + str(s[\"dW1\"]))\n",
    "print(\"s[\\\"db1\\\"] = \" + str(s[\"db1\"]))\n",
    "print(\"s[\\\"dW2\\\"] = \" + str(s[\"dW2\"]))\n",
    "print(\"s[\\\"db2\\\"] = \" + str(s[\"db2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
